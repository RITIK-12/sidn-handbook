{header("From Salience Maps to Mechanistic Interpretability")}

<p>
In this chapter, we pick up our story of interprtability in 2010
and bring it forward to 2024.

<p>
In lower-dimensional settings where input variables have understandable meanings,
both decision trees and GAMs are often used in practice to achieve debuggable,
interpretable machine learned models. It is worth understanding both the classic
approaches and modern research proposals around both these approaches. However,
the field of interpretable machine learning underwent an earthquake in 2012 when
<a href="https://papers.baulab.info/papers/Krizhevsky-2012.pdf">Krizhevsky et al</a>
published the AlexNet neural network that smashed the competitors in the ImageNet
object classification challenge.  At that moment, the community came to the
realization that huge performance gains could be achieved if we were willing to
use models like deep neural networks that were not designed to be interperetable.

<p>
What do do about the interpretability of deep neural networks brings
us to the second main approach to interpretability.

<h2>Approach 2: Sensitivity Analysis (XAI, 2012)</h2>

<p>


<p><a href="https://arxiv.org/abs/1311.2901">Zeiler et al.</a> proposed several extensions to Erhan's
approach using real images rather than a synthesized \( x^* \).  For example, Zeiler visulized
the effects of occluding a small amount of input on hidden features and classifier output,
and he obtained heatmaps that revealed that the networks are very senisitive to small
parts of the input, for example, responding most sharply to the face of a (dog as shown in
the b and d columents below), whie not responding much to masking in other parts of the input.

<img src="images/zeiler-methods.jpg" style="max-width:80%; width:800px;" class="mx-auto d-block">

<h3>Salience Maps</h3>

<p>The local sensitivity of neural networks led to the development of many <em>salience map</em>
methods for quickly estimating and visualizing this local sensitivity in a single fast
calculation.

<p>For example, the method of <em>integrated graidents</em>...


<h3>Sanity Checks</h3>

<p>
It was understood that in a <em>shallow</em> neural network

<p>
In <a href="https://papers.baulab.info/papers/also/Erhan-2009.pdf">2009,
Dumitru Erhan</a> wrote <em>Visualizing Higher-Layer Features of a Deep Network</em>,
where he proposed analyzing the components of a neural network by identifying the
inputs that caused them to maximize their output.  That is, writing
\( h_{ij}( \theta, x) \) as the value of the neuron \( h_{ij} \) within a network
with parameters \( \theta \) in response to the input \( x \), he proposed understanding
the neuron by identifying and visualizing the maximizing input

\[ x^* = \arg\max_{x \text{ s.t. } ||x|| = \rho} h_{ij}(\theta, x) \]

<p>Erhan proposed maximizing input \( x^* \) can be found through
gradient descent.

{footer()}
