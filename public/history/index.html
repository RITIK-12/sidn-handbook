<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Structure and Interpretation of Deep Networks</title>

<!-- Bootstrap CSS Imports -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

<!-- Google Font Imports -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap"
rel="stylesheet">

<!-- Custom CSS Imports -->
<link rel="stylesheet" href="/css/style.css">

<!-- Mathjax -->
<script sid="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js" integrity="sha512-6FaAxxHuKuzaGHWnV00ftWqP3luSBRSopnNAA2RvQH1fOfnF/A1wOfiUWF7cLIOFcfb1dEhXwo5VG3DAisocRw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<!-- jQuery -->
<script src="https://code.jquery.com/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>

<!-- Bootstrap JS -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.min.js" integrity="sha256-3gQJhtmj7YnV1fmtbVcnAV6eI4ws0Tr48bVZCThtCGQ=" crossorigin="anonymous"></script>

<!-- Auto Table of Contents -->
<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css"
/>
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>

</head>

<body data-bs-spy="scroll" data-bs-target="#toc">
<nav class="navbar fixed-top navbar-dark bg-dark">
<div class="container-fluid">
<ul class="navbar-nav mr-auto">
 <li class="nav-item active">
  <a class="nav-link" href="/history/index.html">Three Approaches to Interpretable ML: a History</a>
 </li>
</ul>
<ul class="navbar-nav ml-auto">
 <li class="nav-item">
  <a class="navbar-brand" href="/">Structure and Intrepretation of Deep Networks</a>
 </li>
</ul>
</div>
</nav>

<div class="container mt-5">
<div class="row">
<div class="col-2 position-fixed pt-5">
<nav id="toc" data-toggle="toc"></nav>
</div>
<div class="col-2"></div>
<main class="col-8">
<h1 class="mt-5">Three Approaches to Interpretable ML: a History</h1>


<p>
This course arrives in late 2024, as interest in interpretable machine learning is growing.
Interest is being driven by remarkable gains in capabilities in large-scale generative AI,
especially large transformer language models such as ChatGPT and text-to-image synthesis
diffusion  models such as DallE, and the question of the moment is "how do these
huge neural networks work?"
<p>
Yet the puzzle of interpretable machine learning is inherent to <em>all</em>
machine learning, and so it has challanged computer scientists for many years.
In this chapter, we take an abbreviated walk through of the history of
interpretable machine learning preceding the current generative AI moment.

<h2>The Problem of Interpretability</h2>

<p>The problem of interpretability dates to the earliest days of computing:
<a href="https://papers.baulab.info/papers/also/Turing-1950.pdf"
>his 1950 essay on machine intelligence</a>, Alan Turing wrote about
the challenge of machine learning interpretability:

<blockquote>
An important feature of a learning machine is that its teacher
will often by very largely ignorant of quite what is going on inside,
although he may still be able to some extent to predict his pupil's behavior.
(<a href="https://papers.baulab.info/papers/also/Turing-1950.pdf">p. 458</a>)
</blockquote>

<p>Turing recognized that, Ada Lovelace's prediction that a computer could
only ever "do whatever we know how to order it to perform" was false.
Although just a dream in 1950, he knew that the success of machine learning
would someday lead to computers that could perform tasks that their
makers would not understand.

<p>The lack of interpretability was one reason for the slow uptake of
machine learning methods in early decades of computer science.  For
example, Marvin Minsky was fond of explaining that machine learning
offered <a href="https://arxiv.org/abs/2007.10928"
>no escape from the biases of a programmer</a>, just a way to make
<a href="https://cse-robotics.engr.tamu.edu/dshell/cs625/koan-sussman.pdf"
>uninterpretable programs with <em>unknown</em> biases</a>. Minksy famously
steered AI funding in the 1970s away from machine learning
methods such as
<a href="https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon"
>Rosenblatt's neural network methods</a>, towards "good old fashioined"
symbolic AI that was made of fully human interpretable LISP programs.

<p>But then, by the 1980s, machine learning was becoming both
practical and useful. How to deal with the problem of interpretability?

<p>But then, once machine learning 

<h2>Approach 1: Models Interpretable By Design (1985)</h2>

<p>The community's first answer to ML interpretability
was to avoid "uninterpretable" solutions
like neural networks and instead favor <em>interpretable machine learning</em>.

<p>Here we will point out two of the main approaches to interpretable
ML from this era.

<h3>Decision Trees</h3>

<p>A decision tree is a classification algorithm consisting of a series
of decision steps. Typically, at each step, a single variable is
examined, and a choice between next steps is taken depending on
a threshold value for the variable.  Decision trees
are highly interpretable and are often simple enough that they can
be executed "by hand" by people. In the real world, decision
trees are still often used exactly in this way: for example, here is
the standard high-speed emergency room decision tree called START.
It is designed to be simple enough for EMTs to memorize and apply
in seconds to triage the most serious emergency cases.

<img src="images/start-decision-tree.jpg" style="max-width:30%; width:300px;" class="mx-auto d-block">

<h3>Classic approach: CART trees</h3>

<p>The most popular decision tree learning algorithm is called CART
(<a href="https://www.taylorfrancis.com/books/mono/10.1201/9781315139470/classification-regression-trees-leo-breiman-jerome-friedman-olshen-charles-stone">Brieman, et al, 1984</a>), and it works
by constructing a decision tree with a simple greedy heuristic:

<ol>
<li>For each variable, a threshold is found that maximizes
the "purity" of the separated training samples, where
purity is quantified by the Gini value

\[ 1 - \sum_{k} p_{k}^2 \]

<li>The tree is built by adding a choice and threshold for
the variable that achieves the highest highest Gini index
at each step, with each step subdividing the choices
until the accuracy is as high as you want
</ol>

<p>The resulting decision tree boundaries look like rectilinear planes.
You can see the effect concretely in
<a href="https://colab.research.google.com/github/ageron/handson-ml2/blob/master/06_decision_trees.ipynb"
>this colab notebook, by <a href="https://github.com/ageron/">Aur&eacute;lien Geron</a>,
which illustrates how to use Scikit-learn to run the standard CART decision tree algorithm.

<img src="images/decision-scatter.png" style="max-width:30%; width:300px;" class="mx-auto d-block">

<h3>Current Research: Optimal Decision Trees</h3>

<p>CART decision trees are not optimal; there can be more economical
sets of choices that lead to higher accuracy classifiers than the choices
found by this greedy approach.  Optimal decision tree algorithms remain an
active area of research today. For example, in 2019, 
<a href="https://arxiv.org/abs/1904.12847">Hu, Rudin and Seltzer</a> published
the Optimal Sparse Decision Trees algorithm
(<a href="https://github.com/xiyanghu/OSDT">OSDT github here</a>), 
and followup work by <a href="https://arxiv.org/abs/2209.08040"
>Xin, et al</a> relaxes the search to allow users to choose the
most interpretable tree out of a set of near-optimal choices.
(<A href="https://github.com/ubc-systopia/treeFarms">Github here</a>).


<p>Decision trees are great for human-executable decisionmaking,
but what if we just need the decisions to be human-understandable
without necessarily being human-executable?  Can we do better?

<p>Here I'll briefly mention a second interpretable ML
approach that is popular in industry.

<h3>Generalized Additive Models</h3>

<p><a href="https://projecteuclid.org/journals/statistical-science/volume-1/issue-3/Generalized-Additive-Models/10.1214/ss/1177013604.full">Generalized Additive Models</a>
 (Hastie, et al 1986)</a>
are a type of model that is based on the fact that any multivariate function
can be decomposed into a sum of univariate functions of the input variables \( x_i \):

\[ y = f_1(x_1) + f_2(x_2) + f_3(x_3) + \cdots \]

<p>(This fact is known as the <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem">Kolmogorov Arnold representation theorem</a>.

<p>A sum of univariate functions like this can be very interpretable.
For example, suppose you need to predict somebody's income based on the year,
their age, and their level of education.  Rather than using a decision tree
(which is based on discontinous decisions) or a neural network (which is hard
to understand), you could learn univariate functions \( f_{year}(year) \),
\( f_{age}(age) \) and \( f_{education}(education) \), that, when added up,
make a prediction for \( y \):

<img src="images/gam-example-income.png" style="max-width:80%; width:800px;" class="mx-auto d-block">

<p>As you can see, this kind of model is very interpretable: reading individual
function can directly give you the intuition about how changing one variable
will affect the output.

<h3>Classic Approach: Backfitting Splines</h3>

<p>GAMs were devised by <a href="https://pdodds.w3.uvm.edu/files/papers/others/1986/hastie1986a.pdf"
>Hastie and Tibshirani</a> in 1986, building on a 1985 algorittm by <a href="https://www.jstor.org/stable/2288477">Breiman and Friedman</a> to compute such models.

<p>The 1985 Brieman paper describes the <a href="https://en.wikipedia.org/wiki/Backfitting_algorithm"
>Backfitting algorithm</a> for finding an additive model by beginning with a naive set of \( f_i \),
and then iteratively improving them by distributing the residual error to each function.

<p>Typically the individual functions are constrained to be smooth splines (and sometimes
other constraints are imposed, such as convexity or monotonicitiy of the individual \( f_i \)
components). This both helps avoid overfitting, helps ensure that the resulting model
is more interpretable.

<p><a href="https://colab.research.google.com/github/davidbau/sidn-handbook/blob/main/public/history/colab/tour_of_pygam.ipynb"
>This colab notebook, by the authors of pyGAM</a> demonstrates the pyGAM
package, which applies backfitting and illustrates and the concepts of GAMs very nicely.

<h3>Current Research: Neural Additive Models and KANs</h3>

<p>Recently, <a href="https://arxiv.org/abs/2004.13912">Agrawal, et al</a>
(on a team with Rich Caurana and Geoff Hinton) proposed <em>Neural Additive Models</em>
which are a type of GAM that uses neural networks instead of spline-smoothed
functions for the \( f_i \).

<p>This year, <a href="https://arxiv.org/abs/2404.19756">Liu, et al</a>
(on a team with Max Tegmark) proposed <em>Kolmogorov-Arnold Networks</em> (KANs),
that take the opposite tack: instead of using neural networks to create GAMs,
they use spline-based GAMs as a drop-in for neural network MLPs, claiming
better interpretability and good scaling.

<p>In lower-dimensional settings where input variables have understandable meanings,
both decision trees and GAMs are often used in practice to achieve debuggable,
interpretable machine learned models. It is worth understanding both the classic
approaches and modern research proposals around both these approaches. However,
the field of interpretable machine learning underwent an earthquake in 2012 when
<a href="https://papers.baulab.info/papers/Krizhevsky-2012.pdf">Krizhevsky et al</a>
published the AlexNet neural network that smashed the competitors in the ImageNet
object classification challenge.  At that moment, the community came to the
realization that huge performance gains could be achieved if we were willing to
use models like neural networks that were not designed to be interperetable.

<p>What do do about the interpretability of uninterpretable neural networks brings
us to the second main approach to interpretability.

<h2>Approach 2: Sensitivity Analysis (XAI, 2012)</h2>

<p>In <a href="https://papers.baulab.info/papers/also/Erhan-2009.pdf">2009,
Dumitru Erhan</a> wrote <em>Visualizing Higher-Layer Features of a Deep Network</em>,
where he proposed analyzing the components of a neural network by identifying the
inputs that caused them to maximize their output.  That is, writing
\( h_{ij}( \theta, x) \) as the value of the neuron \( h_{ij} \) within a network
with parameters \( \theta \) in response to the input \( x \), he proposed understanding
the neuron by identifying and visualizing the maximizing input

\[ x^* = \arg\max_{x \text{ s.t. } ||x|| = \rho} h_{ij}(\theta, x) \]

<p>Erhan proposed maximizing input \( x^* \) can be found through
gradient descent.

<p><a href="https://arxiv.org/abs/1311.2901">Zeiler et al.</a> proposed several extensions to Erhan's
approach using real images rather than a synthesized \( x^* \).  For example, Zeiler visulized
the effects of occluding a small amount of input on hidden features and classifier output,
and he obtained heatmaps that revealed that the networks are very senisitive to small
parts of the input, for example, responding most sharply to the face of a (dog as shown in
the b and d columents below), whie not responding much to masking in other parts of the input.

<img src="images/zeiler-methods.jpg" style="max-width:80%; width:800px;" class="mx-auto d-block">

<h3>Salience Maps</h3>

<p>The local sensitivity of neural networks led to the development of many <em>salience map</em>
methods for quickly estimating and visualizing this local sensitivity in a single fast
calculation.

<p>For example, the method of <em>integrated graidents</em>...


<h3>Sanity Checks</h3>



</main>
</div>
</div>
</body>
</html>

