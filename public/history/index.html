<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Structure and Interpretation of Deep Networks</title>

<!-- Bootstrap CSS Imports -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

<!-- Google Font Imports -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap"
rel="stylesheet">

<!-- Custom CSS Imports -->
<link rel="stylesheet" href="/css/style.css">

<!-- Mathjax -->
<script sid="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js" integrity="sha512-6FaAxxHuKuzaGHWnV00ftWqP3luSBRSopnNAA2RvQH1fOfnF/A1wOfiUWF7cLIOFcfb1dEhXwo5VG3DAisocRw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

</head>

<body>
<nav class="navbar navbar-dark bg-dark">
<div class="container-fluid">
<ul class="navbar-nav mr-auto">
 <li class="nav-item active">
  <a class="nav-link" href="/history/index.html">Three kinds of interpretability in machine learning</a>
 </li>
</ul>
<ul class="navbar-nav ml-auto">
 <li class="nav-item">
  <a class="navbar-brand" href="/">Structure and Intrepretation of Deep Networks</a>
 </li>
</ul>
</div>
</nav>

<div class="container">
<main>
<h1 class="mt-5">Three kinds of interpretability in machine learning</h1>


<p>
This course arrives in late 2024, as interest in interpretable machine learning is growing.
Interest is being driven by remarkable gains in capabilities in large-scale generative AI,
especially large transformer language models such as ChatGPT and text-to-image synthesis
diffusion  models such as DallE, and the question of the moment is "how do these
huge neural networks work?"
<p>
Yet the puzzle of interpretable machine learning is inherent to this type of
programming and has faced computer scientists for many years.
In this chapter, we briefly take a moment to retrace some of the history of
interpretable machine learning that precedes the current generative AI moment.

<h2>The Problem of Interpretability</h2>

<p>The problem of interpretability dates to the earliest days of computing:
<a href="https://papers.baulab.info/papers/also/Turing-1950.pdf"
>his 1950 essay on machine intelligence</a>, Alan Turing wrote about
the challenge of machine learning interpretability:

<blockquote>
An important feature of a learning machine is that its teacher
will often by very largely ignorant of quite what is going on inside,
although he may still be able to some extent to predict his pupil's behavior.
(<a href="https://papers.baulab.info/papers/also/Turing-1950.pdf">p. 458</a>)
</blockquote>

<p>Turing recognized that, Ada Lovelace's prediction that a computer could
only ever "do whatever we know how to order it to perform" was false.
Although just a dream in 1950, he knew that the success of machine learning
would someday lead to computers that could perform tasks that their
makers would not understand.

<p>The lack of interpretability was one reason for the slow uptake of
machine learning methods in early decades of computer science.  For
example, Marvin Minsky was fond of explaining that machine learning
offered <a href="https://arxiv.org/abs/2007.10928"
>no escape from the biases of a programmer</a>, just a way to make
<a href="https://cse-robotics.engr.tamu.edu/dshell/cs625/koan-sussman.pdf"
>uninterpretable programs with <em>unknown</em> biases</a>. Minksy famously
steered AI funding in the 1970s away from machine learning
methods such as
<a href="https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon"
>Rosenblatt's neural network methods</a>, towards "good old fashioined"
symbolic AI that was made of fully human interpretable LISP programs.

<p>But then, by the 1980s, machine learning was becoming both
practical and useful. How to deal with the problem of interpretability?

<p>But then, once machine learning 

<h2>Solution 1: Models Interpretable By Design</h2>

<p>The community's first answer to ML interpretability
was to avoid "uninterpretable" solutions
like neural networks and instead favor <em>interpretable machine learning</em>.

<p>Here we will point out two of the main approaches to interpretable
ML from this era.

<h3>Decision Trees</h3>

<p>A decision tree is a classification algorithm consisting of a series
of decision steps. Typically, at each step, a single variable is
examined, and a choice between next steps is taken depending on
a threshold value for the variable.  Decision trees
are highly interpretable and are often simple enough that they can
be executed "by hand" by people. In the real world, decision
trees are still often used exactly in this way: for example, here is
the standard high-speed emergency room decision tree called START.
It is designed to be simple enough for EMTs to memorize and apply
in seconds to triage the most serious emergency cases.

<img src="/images/start-decision-tree.jpg" style="max-width:30%; width:300px;" class="mx-auto d-block">

<h4>Classic approach: CART trees</h4>

<p>The most popular decision tree learning algorithm is called CART
(<a href="https://www.taylorfrancis.com/books/mono/10.1201/9781315139470/classification-regression-trees-leo-breiman-jerome-friedman-olshen-charles-stone">Brieman, et al, 1984</a>), and it works
by constructing a decision tree with a simple greedy heuristic:

<ol>
<li>For each variable, a threshold is found that maximizes
the "purity" of the separated training samples, where
purity is quantified by the Gini value

\[ 1 - \sum_{k} p_{k}^2 \]

<li>The tree is built by adding a choice and threshold for
the variable that achieves the highest highest Gini index
at each step, with each step subdividing the choices
until the accuracy is as high as you want
</ol>

<p>The resulting decision tree boundaries look like rectilinear planes.
You can see the effect concretely in
<a href="https://colab.research.google.com/github/ageron/handson-ml2/blob/master/06_decision_trees.ipynb"
>this colab notebook, by <a href="https://github.com/ageron/">Aur&eacute;lien Geron</a>,
which illustrates how to use Scikit-learn to run the standard CART decision tree algorithm.

<img src="/images/decision-scatter.png" style="max-width:30%; width:300px;" class="mx-auto d-block">

<h4>Current Research: Optimal Decision Trees</h4>

<p>CART decision trees are not optimal; there can be more economical
sets of choices that lead to higher accuracy classifiers than the choices
found by this greedy approach.  Optimal decision tree algorithms remain an
active area of research today. For example, in 2019, 
<a href="https://arxiv.org/abs/1904.12847">Hu, Rudin and Seltzer</a> published
the Optimal Sparse Decision Trees algorithm
(<a href="https://github.com/xiyanghu/OSDT">OSDT github here</a>), 
and followup work by <a href="https://arxiv.org/abs/2209.08040"
>Xin, et al</a> relaxes the search to allow users to choose the
most interpretable tree out of a set of near-optimal choices.
(<A href="https://github.com/ubc-systopia/treeFarms">Github here</a>).


<p>Decision trees are great for human-executable decisionmaking,
but what if we just need the decisions to be human-understandable
without necessarily being human-executable?  Can we do better?

<p>Here I'll briefly mention a second interpretable ML
approach that is popular in industry.

<h3>Generalized Additive Models</h3>

<p><a href="https://projecteuclid.org/journals/statistical-science/volume-1/issue-3/Generalized-Additive-Models/10.1214/ss/1177013604.full">Generalized Additive Models</a>
 (Hastie, et al 1986)</a>
are a type of model that is based on the fact that any multivariate function
can be decomposed into a sum of univariate functions of the input variables \( x_i \):

\[ y = f_1(x_1) + f_2(x_2) + f_3(x_3) + \cdots \]

For example, suppose you need to predict somebody's income based on the year,
their age, and their level of education.  Rather than using a decision tree
(which is based on discontinous decisions) or a neural network (which is hard
to understand), you could learn univariate functions \( f_{year}(year) \),
\( f_{age}(age) \) and \( f_{education}(education) \), that, when added up,
make a prediction for \( y \):

<img src="/images/gam-example-income.png" style="max-width:80%; width:800px;" class="mx-auto d-block">

As you can see, this kind of model is very interpretable: reading individual
function can directly give you the intuition about how changing one variable
will affect the output.

GAMs were devised by <a href="https://www.jstor.org/stable/2288477">Breiman and Friedman</a>:
their original 1985 paper describes the <a href="https://en.wikipedia.org/wiki/Backfitting_algorithm"
>Backfitting algorithm</a> for finding a GAM model by beginning with a naive set of \( f_i \),
and then iteratively improving them by distributing the residual error to each function.

Typically the individual functions are constrained to be smooth splines (and sometimes
other constraints are imposed) to avoid overfitting, and so that the reusults are more
interpretable.

<a href="">This colab notebook, by the authors of pyGAM</a> demonstrates the pyGAM
package and the concepts of GAMs very nicely.


</main>
</div>
</body>
</html>

