<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Structure and Interpretation of Deep Networks</title>

<!-- Bootstrap CSS Imports -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

<!-- Google Font Imports -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap"
rel="stylesheet">

<!-- Custom CSS Imports -->
<link rel="stylesheet" href="/css/style.css">

<!-- Mathjax -->
<script sid="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js" integrity="sha512-6FaAxxHuKuzaGHWnV00ftWqP3luSBRSopnNAA2RvQH1fOfnF/A1wOfiUWF7cLIOFcfb1dEhXwo5VG3DAisocRw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<!-- jQuery -->
<script src="https://code.jquery.com/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>

<!-- Bootstrap JS -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.min.js" integrity="sha256-3gQJhtmj7YnV1fmtbVcnAV6eI4ws0Tr48bVZCThtCGQ=" crossorigin="anonymous"></script>

<!-- Auto Table of Contents -->
<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css"
/>
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>

</head>

<body data-bs-spy="scroll" data-bs-target="#toc">
<nav class="navbar fixed-top navbar-dark bg-dark">
<div class="container-fluid">
<ul class="navbar-nav mr-auto">
 <li class="nav-item active">
  <a class="nav-link" href="/history/index.html">A History of Interpretable Machine Learning</a>
 </li>
</ul>
<ul class="navbar-nav ml-auto">
 <li class="nav-item">
  <a class="navbar-brand" href="/">Structure and Intrepretation of Deep Networks</a>
 </li>
</ul>
</div>
</nav>

<div class="container mt-5">
<div class="row">
<div class="col-2 position-fixed pt-5">
<nav id="toc" data-toggle="toc"></nav>
</div>
<div class="col-2"></div>
<main class="col-8">
<h1 class="mt-5">A History of Interpretable Machine Learning</h1>


<p>
This course arrives in late 2024, as interest in interpretable machine learning is growing.
Interest is being driven by remarkable gains in capabilities in large-scale generative AI,
especially large transformer language models such as ChatGPT and text-to-image synthesis
diffusion  models such as DallE. The question of the moment is "how do these
huge generative neural networks work?"

<p>
Yet the puzzle of interpretability is inherent to <em>all</em>
machine learning, and the problem has challanged computer scientists
for many years.  In this chapter, we take a moment to trace the hundred
years of intellectual debuate about interpretable machine learning
that have preceding the current generative moment in AI.

<h2>From Lovelace to Turing to Minksy and Wolpert</h2>

<p>
Deeply ingrained in our conception of computer science is the
belief that programming is about creating human-designed
algorithms that reflect the insights and understanding of
the programmer. This notion was famously articulated by
Ada Lovelace, the very first programmer in history, who
designed programs for the Babbage analytical engine.

<blockquote>
The Analytical Engine has no pretensions whatever to <b>originate</b> anything.
It can do whatever <b>we know</b> how to order it to perform.
</blockquote>

<p>
Yet Alan Turing disagreed that computer science would be
inherently tethered to human knowledge. He argued that some day
it would be possible to teach machines how to <em>learn</em> automatically.
<a href="https://papers.baulab.info/papers/also/Turing-1950.pdf"
>In his 1950 essay on machine intelligence</a>, Alan Turing
pointed out that machine larning would immediately lead the challenge
of machine learning interpretability:

<blockquote>
An important feature of a learning machine is that its teacher
will often by very largely ignorant of quite what is going on inside,
although he may still be able to some extent to predict his pupil's behavior.
(<a href="https://papers.baulab.info/papers/also/Turing-1950.pdf">p. 458</a>)
</blockquote>

<p>
The inherent human ignorance about the internals of machine-learned
systems was one reason for the slow uptake of
machine learning methods in early decades of computer science.

<p>
For those who might believe that human ignorance is a <em>good</em>
thing, it is important for ML scientists to be aware of the
<a href="https://arxiv.org/abs/2007.10928">No-Free-Lunch Theorems</a>
by Wolpert and Macready.  The NFL theorems proves that there no
learning algorithm that generalizes well for all possible data.
In other words, learning algorithms only succeed due to the biases
that are reflected within the algorithm. They must be specialized to
particular kinds of problems reflecting the needs and assumptions
the programmer creating the learning algorithm.

<p>
This intuition about the irreducible nature of the responsibility
of the programmer famously led Marvin Minksy to steer AI funding
in the 1970s away from machine learning methods such as
<a href="https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon"
>Rosenblatt's neural network methods</a>, towards "good old fashioined"
symbolic AI that used LISP.  Minsky believed it was important to
put human intention, logic, and interpretability
front and center of the development of artificial intelligence.

<p>Marvin Minsky is famous for the following parable about
a his advice to a young Jerry Sussman:

<blockquote>
<p>In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6.
<p>"What are you doing?", asked Minsky.
<p>"I am training a randomly wired neural net to play Tic-Tac-Toe" Sussman replied.
<p>"Why is the net wired randomly?", asked Minsky.
<p>"I do not want it to have any preconceptions of how to play‚Äù, Sussman said.
<p>Minsky then shut his eyes.
<p>"Why do you close your eyes?", Sussman asked his teacher.
<p>"So that the room will be empty."
<p>At that moment, Sussman was enlightened.
</blockquote>

<p>
Yet in the end, it proved to be Minsky, not the machine learning
programmers, who were closing their eyes to the real challenge of
interpretability.  By the 1980s, machine learning was becoming
both practical and useful, leaving computer scientists with the
unsolved puzzle of how to understand self-programmed
machine-learned systems.

<h2>Approach 1: Models Interpretable By Design (1985)</h2>

<p>The community's first answer to ML interpretability
was to make the problem easier by avoiding "uninterpretable"
learning algorithms like neural networks and instead favor
<em>interpretable machine learning</em> methods.

<p>As an example, we will point out two of the main approaches to
interpretable ML that date from the 1980s that are still
widely used today: Decision Trees and Generalized Additive Models.
Both these ideas sprang from learning algorithms devised by
<a href="https://en.wikipedia.org/wiki/Leo_Breiman">Leo Breiman</a>.

<h3>Decision Trees</h3>

<p>A decision tree is a classification algorithm consisting of a series
of decision steps. Typically, at each step, a single variable is
examined, and a choice between next steps is taken depending on
a threshold value for the variable.  Decision trees
are highly interpretable and are often simple enough that they can
be executed "by hand" by people. In the real world, decision
trees are still often used exactly in this way: for example, here is
the standard high-speed emergency room decision tree called START.
It is designed to be simple enough for EMTs to memorize and apply
in seconds to triage the most serious emergency cases.

<img src="images/start-decision-tree.jpg" style="max-width:30%; width:300px;" class="mx-auto d-block">

<h3>Classic approach: CART trees</h3>

<p>
The most popular decision tree learning algorithm is called CART
(<a href="https://www.taylorfrancis.com/books/mono/10.1201/9781315139470/classification-regression-trees-leo-breiman-jerome-friedman-olshen-charles-stone">Brieman, et al, 1984</a>), and it works
by constructing a decision tree with a simple greedy heuristic:

<ol>
<li>For each input variable, a threshold is found that maximizes
the "purity" of the separated training samples, where
purity is quantified by the Gini value (in which \( p_k \) denotes
the portion of a new partition that is occupied by class \( k \)).

\[ 1 - \sum_{k} p_{k}^2 \]

<li>The tree is built by adding a choice and threshold for
the variable that achieves the highest highest Gini index
at each step, with each step subdividing the choices
until the accuracy is as high as you want
</ol>

<p>
The resulting decision tree boundaries look like rectilinear planes.
You can see the effect concretely in
<a href="https://colab.research.google.com/github/ageron/handson-ml2/blob/master/06_decision_trees.ipynb"
>this colab notebook, by <a href="https://github.com/ageron/">Aur&eacute;lien Geron</a>,
which illustrates how to use Scikit-learn to run the standard CART decision tree algorithm.

<img src="images/decision-scatter.png" style="max-width:30%; width:300px;" class="mx-auto d-block">

<h3>Current Research: Optimal Decision Trees</h3>

<p>
CART decision trees are not optimal; there can be more economical
sets of choices that lead to higher accuracy classifiers than the choices
found by this greedy approach.  Optimal decision tree algorithms remain an
active area of research today. For example, in 2019, 
<a href="https://arxiv.org/abs/1904.12847">Hu, Rudin and Seltzer</a> published
the Optimal Sparse Decision Trees algorithm
(<a href="https://github.com/xiyanghu/OSDT">OSDT github here</a>), 
and followup work by <a href="https://arxiv.org/abs/2209.08040"
>Xin, et al</a> relaxes the search to allow users to choose the
most interpretable tree out of a set of near-optimal choices.
(<A href="https://github.com/ubc-systopia/treeFarms">Github here</a>).

<p>
Decision trees are great for human-executable decisionmaking,
but what if we just need the decisions to be human-understandable
without necessarily being human-executable?  Can we do better?

<p>
Here I'll briefly mention a second interpretable ML
approach that is popular in industry.

<h3>Generalized Additive Models</h3>

<p>
<a href="https://projecteuclid.org/journals/statistical-science/volume-1/issue-3/Generalized-Additive-Models/10.1214/ss/1177013604.full">Generalized Additive Models</a>
 (Hastie, et al 1986)</a>
are a type of model that is based on the fact that any multivariate function
can be decomposed into a sum of univariate functions of the input variables \( x_i \):

\[ y = f_1(x_1) + f_2(x_2) + f_3(x_3) + \cdots \]

<p>
(This fact is known as the <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem">Kolmogorov Arnold representation theorem</a>.

<p>
A sum of univariate functions like this can be very interpretable.
For example, suppose you need to predict somebody's income based on the year,
their age, and their level of education.  Rather than using a decision tree
(which is based on discontinous decisions) or a neural network (which is hard
to understand), you could learn univariate functions \( f_{year}(year) \),
\( f_{age}(age) \) and \( f_{education}(education) \), that, when added up,
make a prediction for \( y \):

<img src="images/gam-example-income.png" style="max-width:80%; width:800px;" class="mx-auto d-block">

<p>
As you can see, this kind of model is very interpretable: reading individual
function can directly give you the intuition about how changing one variable
will affect the output.

<h3>Classic Approach: Backfitting Splines</h3>

<p>
GAMs were devised by <a href="https://pdodds.w3.uvm.edu/files/papers/others/1986/hastie1986a.pdf"
>Hastie and Tibshirani</a> in 1986, building on a 1985 algorittm by <a href="https://www.jstor.org/stable/2288477">Breiman and Friedman</a> to compute such models.

<p>
The 1985 Brieman paper describes the <a href="https://en.wikipedia.org/wiki/Backfitting_algorithm"
>Backfitting algorithm</a> for finding an additive model by beginning with a naive set of \( f_i \),
and then iteratively improving them by distributing the residual error to each function.

<p>
Typically the individual functions are constrained to be smooth splines (and sometimes
other constraints are imposed, such as convexity or monotonicitiy of the individual \( f_i \)
components). This both helps avoid overfitting, helps ensure that the resulting model
is more interpretable.

<p>
<a href="https://colab.research.google.com/github/davidbau/sidn-handbook/blob/main/public/history/colab/tour_of_pygam.ipynb"
>This colab notebook, by the authors of pyGAM</a> demonstrates the pyGAM
package, which applies backfitting and illustrates and the concepts of GAMs very nicely.

<h3>Current Research: Neural Additive Models and KANs</h3>

<p>
Recently, <a href="https://arxiv.org/abs/2004.13912">Agrawal, et al</a>
(on a team with Rich Caurana and Geoff Hinton) proposed <em>Neural Additive Models</em>
which are a type of GAM that uses neural networks instead of spline-smoothed
functions for the \( f_i \).

<p>
This year, <a href="https://arxiv.org/abs/2404.19756">Liu, et al</a>
(on a team with Max Tegmark) proposed <em>Kolmogorov-Arnold Networks</em> (KANs),
that take the opposite tack: instead of using neural networks to create GAMs,
they use spline-based GAMs as a drop-in for neural network MLPs, claiming
better interpretability and good scaling.

<p>
In lower-dimensional settings where input variables have understandable meanings,
both decision trees and GAMs are still used in practice to achieve debuggable,
interpretable machine learned models. It is worth understanding both the classic
approaches and modern research proposals around both these approaches, because
they exemplify what it means for a learned system to be interpretable by
humans.

<p>
However, as data volumes grew in the late 1990s with the growth of the internet
and the digitization of society, it became clear that these early interpretable
machine learning approaches did not scale as well as some more opaque methods.
For example, in natural language modeling, advocates of decision trees
(see <a href="https://www.sciencedirect.com/science/article/pii/S0167639398000181"
>Potamianos</a>) were reporting negative results compared to ngram models.

<p>
The field of interpretable machine learning underwent an earthquake in 2012 when
<a href="https://papers.baulab.info/papers/Krizhevsky-2012.pdf">Krizhevsky et al</a>
published the AlexNet neural network that smashed the competitors in the ImageNet
object classification challenge.  At that moment, the community came to the
realization that huge performance gains could be achieved if we were willing to
use models like deep neural networks that have many layeers of internal
variables that are not designed to be interperetable.

<p>
What do do about the interpretability of deep neural networks brings
us to the second part of the history, and the second main approach to
interpretability.


</main>
</div>
</div>
</body>
</html>

