<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Structure and Interpretation of Deep Networks</title>

<!-- Bootstrap CSS Imports -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

<!-- Google Font Imports -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap"
rel="stylesheet">

<!-- Custom CSS Imports -->
<link rel="stylesheet" href="/css/style.css">

<!-- Mathjax -->
<script sid="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js" integrity="sha512-6FaAxxHuKuzaGHWnV00ftWqP3luSBRSopnNAA2RvQH1fOfnF/A1wOfiUWF7cLIOFcfb1dEhXwo5VG3DAisocRw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<!-- jQuery -->
<script src="https://code.jquery.com/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>

<!-- Bootstrap JS -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.min.js" integrity="sha256-3gQJhtmj7YnV1fmtbVcnAV6eI4ws0Tr48bVZCThtCGQ=" crossorigin="anonymous"></script>

<!-- Auto Table of Contents -->
<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css"
/>
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>

</head>

<body data-bs-spy="scroll" data-bs-target="#toc">
<nav class="navbar navbar-dark bg-dark">
<div class="container-fluid">
<ul class="navbar-nav mr-auto">
 <li class="nav-item active">
  <a class="nav-link" href="/neurons/index.html">Neuron Interpretation</a>
 </li>
</ul>
<ul class="navbar-nav ml-auto">
 <li class="nav-item">
  <a class="navbar-brand" href="/">Structure and Interpretation of Deep Networks</a>
 </li>
</ul>
</div>
</nav>

<div class="container">
<div class="row">
<div class="col-2 position-fixed pt-5">
<nav id="toc" data-toggle="toc"></nav>
</div>
<div class="col-2"></div>
<main class="col-8">
<h1 class="mt-5">Neuron Interpretation</h1>


<!--
<p>
An ideal explanation for a network's decision would deal with
higher-level variables than raw inputs such as pixels. Rather than explaining
a "dog image" by pointing out any individual pixel or even a set of pixels,
an ideal explanation might point "there are four legs here", "there is a dark
nose here"; "there is a floppy ear there"; "there is some curly fur there."
-->
<div><h5>September 12, 2024 • <em>Sheridan Feucht, David Bau</em></h5>

<p>
By definition, deep neural networks consist of layers upon layers of activations, 
where the inputs to each layer are the outputs of the previous layer. 
As any given neuron will have different numerical values for different model inputs, 
it stands to reason that the individual neurons at each layer might represent 
"meaningful" input variables for subsequent layers of the network. 
Analyzing these values constitutes a second early line of work
in interpretability, <em>neuron interpretation</em>. Let's look at some of the ways people have 
approached this problem. 

<h3>What Exactly is a Neuron?</h3>
For feed-forward networks, a "neuron" is just a single unit in a network that takes in inputs 
from multiple other units and outputs a scalar value (kind of like a biological neuron). 
Let's say we have a 3x3 matrix of weights \( W^{\ell} \) in a toy deep network, as well as 
activations from the previous layer \( a^{\ell-1} \): 

\[
W^{\ell} = 
\begin{bmatrix}
0.34 & -0.02 & 1.01\\
-1.22 & -0.03 & 3.05\\
-3.33 & -4.50 & -0.92\\
\end{bmatrix}
\hspace{0.5cm}
a^{\ell-1} = \begin{bmatrix}
0.2 \\
0.4 \\
0.6 \\ 
\end{bmatrix}
\]

Normally, we calculate all of the neurons at once and apply an activation function like
ReLU to get the next set of activations \( a_{\ell}\). 
<!-- array([ 0.666,  1.574, -3.018]) -->
<!-- tensor([0.6660, 1.5740, 0.0000], dtype=torch.float64) -->

\[
\text{ReLU}(W^{\ell}a^{\ell-1}) = 
\begin{bmatrix}
0.67 \\
1.57 \\
0.00 \\ 
\end{bmatrix}
\]

But we can also focus on a single neuron, which would simply correspond to a row 
in the weight matrix \( W^{\ell} \). 
For example, the first neuron of \( W^{\ell} \) looks like the below equation. 
It weights all of the values of \( a^{\ell-1} \), the neuron activations from the previous layer, 
and applies an activation function to ultimately output its own activation.
\[ \text{ReLU}[(0.34 * 0.2) + (-0.02 * 0.4) + (1.01 * 0.6)] = 0.666   \]

For convolutional networks, the term "neuron" is a little different. The general 
idea remains the same: a "neuron" is a unit that takes in all of the values from the previous layer
and outputs a value of its own. However, this entry corresponds a particular <i>filter</i> in the convolutional model, 
whose output is a feature map across the input image, not a scalar. 
Instead of computing a weighted sum of the previous neuron activations, a filter
convolves over all of the previous neuron activations (with a different learned kernel for each channel) 
and sums those maps to produce its output. 
There is a little bit of disagreement with this terminology, however—
some researchers use the term "channel" to describe these feature 
maps and describe specific (x,y) positions within the maps as "neurons." 
But the papers we are reading today mostly use the former definition. 
<br><br>

<h2>Maximal-Response Visualizations of Neurons</h2>

<p>
One of the most common ways to understand the role of a neuron
is to ask the question: "what input causes this neuron to fire most strongly?"

<p>
In <a href="https://papers.baulab.info/papers/also/Erhan-2009.pdf">2009,
Dumitru Erhan</a> wrote <em>Visualizing Higher-Layer Features of a Deep Network</em>,
where he proposed analyzing the components of a neural network by identifying the
inputs that caused them to maximize their output. That is, writing
\( h_{ij}( \theta, x) \) as the value of the neuron \( h_{ij} \) within a network
with parameters \( \theta \) in response to the input \( x \), he proposed understanding
the neuron by identifying and visualizing the maximizing input

\[ x^* = \arg\max_{x \text{ s.t. } ||x|| = \rho} h_{ij}(\theta, x) \]

<p>
Erhan proposed maximizing input \( x^* \) through
gradient descent. By starting with a random or arbitrary \( x \),
optimizing towards \( x^* \) could reveal an image that the
neuron is looking for, a picture of the "concept" for \( h_{ij} \). When this 
optimization was done on a network trained to complete MNIST, they found that 
individual neurons at early layers (left) seemed to select for lines and curves in 
particular positions, whereas late-layer neurons were activated by entire digits. 


<img src="images/mnist_dbn_erhan.png" style="max-width:70%; width:500px;" class="mx-auto d-block">

<p>
Several modifications to the optimization algorithm were proposed by a series
of papers; an excellent feature visualization method that incorporates
many of the techniques was developed by <a href="https://distill.pub/2017/feature-visualization/"
>Chris Olah (2017)</a>, and was used to visualize all the units of several
networks at the <a href="https://microscope.openai.com/models">OpenAI Microscope</a>
website.

<h2>Network Dissection</h2>

Instead of optimizing for inputs that activate a neuron strongly, we can also 
analyze features of images that highly activate a given neuron. 
<a href="https://www.pnas.org/doi/full/10.1073/pnas.1907375117">This paper</a> 
from David's PhD work looks at neurons in two types of vision models: 
a CNN trained on scene classification and a GAN trained to generate scenes. 

<h3>Neurons in a CNN Scene Classifier</h3>
First, the authors look at neurons within a VGG-16 scene classifier. They are able 
to visualize which parts of an image are relevant to a particular filter by looking 
at areas of the image for which the filter activates strongly (beyond its top 1% quantile level). 
To interpret this information, they use a separate image segmentation model to 
predict the presence of a concept \( c \) at any given position. These human-interpretable 
concepts are matched to specific neurons by calculating the IoU between the segmentation 
model and the sections of image that highly activate the filter. 

<img src="images/person-top.png" style="max-width:70%; width:500px;" class="mx-auto d-block">

Using this method, they are able to describe neurons in later convolutional layers 
as "object detectors" that consistently fire on things like planes, heads, and fur. 
The more interpretable these features are, the more important they are shown to be 
for downstream scene classification. They show some interesting experiments in their 
Figure 2 that illustrate what happens when you zero-ablate specific units. For example, 
removing a single unit that corresponds to "snow" decreases accuracy for the class "ski resort" 
by 9-10%. 
<br><br>


<h3>Neurons in a Generative Network</h3>

The authors then analyze a Progressive GAN in a similar way, by looking at the 
regions of the generated image that filters are most active for (beyond their top 1% quantile level). 
While the VGG-16 scene classifier had lots of "object detector" neurons, this GAN 
has a higher proportion of "parts" neurons (like tops of chairs, bottoms of windows, etc), 
which are concentrated near the beginning of the network. 
In a Progressive GAN trained on church scenes, some of these neurons seemed to correspond to trees. 
By zero-ablating twenty of these units, they were able to remove more than half of the generated 
tree pixels from the GAN's output image. 

<img src="images/tree-removal.png" style="max-width:70%; width:500px;" class="mx-auto d-block">

Additionally, they find that <em>activating</em> units in specific locations within an image 
causes the model to draw, for example, extra doors. Interestingly, drawing in doors only works 
in contexts that are semantically reasonable (i.e. activating "door" neurons in the sky does not 
cause a door to be drawn in the sky). 
<br><br>

<h2>Multimodal Neurons</h2>
<p>

The inspiration for the next paper, <a href="https://distill.pub/2021/multimodal-neurons/">Multimodal Neurons in Artificial Neural Networks</a>
comes from a <a href="https://amygdala.psychdept.arizona.edu/IntroData/Readings/week5/Quiroga-reddy-kreiman-koch-Fried+invariant-visual-single-neurons-human+Nature+2005.pdf">2005 neuroscience paper by Quiroga et al.</a>
In the original neuroscience paper, the authors claim to have found specific neurons in the medial temporal lobe that selectively 
fire in response to certain individuals, invariant to the visual features of those stimuli. For example, one neuron 
they studied in the hippocampus would fire in response to pictures of Halle Berry as Catwoman, drawings of her, 
and the literal text "Halle Berry," but not fire in response to other famous actors. 

<p>
In this paper, Goh et al. claim to have found the equivalent phenomenon in CLIP neurons. 
Instead of Halle Berry or Jennifer Aniston, their canonical example is a 
<a href="https://microscope.openai.com/models/contrastive_4x/image_block_4_5_Add_6_0/550">Spider-Man neuron</a>
in the final layer of CLIP ResNet-50 4x. To show this, they use two approaches similar to what we've discussed so far: 
<b>feature visualization</b> across an entire channel and for a particular position in the image (left), and
collecting <b>dataset examples</b> that cause the neuron to fire. 

<img src="images/spiderman-neuron.png" style="max-width:100%; width:800px;" class="mx-auto d-block">

<p>
There are a lot of interesting neuron examples in this paper, along with experiments that 
find language embeddings to match specific "emotion neurons," but the exact reasons for the 
emergence of these multimodal representations is left implied.

<img src="images/emotion-text.png" style="max-width:100%; width:500px;" class="mx-auto d-block">

<p>
<i>Sheridan's opinion:</i> It's worth emphasizing that this paper focuses on CLIP models, 
which are pretrained with the objective of aligning images 
to their textual captions. CLIP models <em>have</em> to map images to high-level semantic 
features in order to succeed on their training objective. It's definitely surprising that 
some of these features seem to be localized to one neuron, but I personally find it 
a little bit fallacious to frame these results as analogous to "Halle Berry" neurons in the brain,
which themselves are somewhat contentious. 
One of the original authors of that paper, Gabriel Kreiman, 
now doubts the veracity of this line of work (<a href="https://klab.tch.harvard.edu/publications/PDFs/gk7786.pdf">Kreiman, 2019</a>; see Fig. 1).
While I don't agree with Gabriel's entire dismissal of category-selective responses, 
I think that we both share similar doubts as to how well humans can look at the 
things a neuron responds to and accurately label what that neuron "wants." 

<br><br> 

<h2>Neurons in a Haystack</h2>

todo 
<br>
<br>

<h2>Demo: TODO</h2>

Forthcoming! 


</main>
</div>
</div>
</body>
</html>

