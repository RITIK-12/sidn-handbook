<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Structure and Interpretation of Deep Networks</title>

<!-- Bootstrap CSS Imports -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

<!-- Google Font Imports -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap"
rel="stylesheet">

<!-- Custom CSS Imports -->
<link rel="stylesheet" href="/css/style.css">

<!-- Mathjax -->
<script sid="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js" integrity="sha512-6FaAxxHuKuzaGHWnV00ftWqP3luSBRSopnNAA2RvQH1fOfnF/A1wOfiUWF7cLIOFcfb1dEhXwo5VG3DAisocRw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<!-- jQuery -->
<script src="https://code.jquery.com/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>

<!-- Bootstrap JS -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.min.js" integrity="sha256-3gQJhtmj7YnV1fmtbVcnAV6eI4ws0Tr48bVZCThtCGQ=" crossorigin="anonymous"></script>

<!-- Auto Table of Contents -->
<link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css"
/>
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>

</head>

<body data-bs-spy="scroll" data-bs-target="#toc">
<nav class="navbar navbar-dark bg-dark">
<div class="container-fluid">
<ul class="navbar-nav mr-auto">
 <li class="nav-item active">
  <a class="nav-link" href="/neurons/index.html">Neuron Interpretation</a>
 </li>
</ul>
<ul class="navbar-nav ml-auto">
 <li class="nav-item">
  <a class="navbar-brand" href="/">Structure and Interpretation of Deep Networks</a>
 </li>
</ul>
</div>
</nav>

<div class="container">
<div class="row">
<div class="col-2 position-fixed pt-5">
<nav id="toc" data-toggle="toc"></nav>
</div>
<div class="col-2"></div>
<main class="col-8">
<h1 class="mt-5">Neuron Interpretation</h1>


<!--
<p>
An ideal explanation for a network's decision would deal with
higher-level variables than raw inputs such as pixels. Rather than explaining
a "dog image" by pointing out any individual pixel or even a set of pixels,
an ideal explanation might point "there are four legs here", "there is a dark
nose here"; "there is a floppy ear there"; "there is some curly fur there."
-->
<div><h5>September 12, 2024 â€¢ <em>Sheridan Feucht, David Bau</em></h5>

<p>
By definition, deep neural networks consist of layers upon layers of <b>neuron activations</b>, 
where the inputs to each layer are the outputs of the previous layer. When we talk about
a particular "neuron," we are referring to a single scalar entry in the vector of activations at a given layer. 
As this neuron will have different numerical values for different model inputs, 
it stands to reason that the individual neurons at each layer might represent 
"meaningful" input variables for subsequent layers of the network. 

<p>
Analyzing these values under different circumstances constitutes a second early line of work
in interpretability, <em>neuron interpretation</em>, where researchers examined individual
neurons to try to discern their meaning. Let's look at some of the ways people have 
approached this problem. 

<h2>Prelude: Maximal-Response Visualizations of Neurons</h2>

<p>
One of the most common ways to understand the role of a neuron
is to ask the question: "what input causes this neuron to fire most strongly?"

<p>
In <a href="https://papers.baulab.info/papers/also/Erhan-2009.pdf">2009,
Dumitru Erhan</a> wrote <em>Visualizing Higher-Layer Features of a Deep Network</em>,
where he proposed analyzing the components of a neural network by identifying the
inputs that caused them to maximize their output. That is, writing
\( h_{ij}( \theta, x) \) as the value of the neuron \( h_{ij} \) within a network
with parameters \( \theta \) in response to the input \( x \), he proposed understanding
the neuron by identifying and visualizing the maximizing input

\[ x^* = \arg\max_{x \text{ s.t. } ||x|| = \rho} h_{ij}(\theta, x) \]

<p>
Erhan proposed maximizing input \( x^* \) through
gradient descent. By starting with a random or arbitrary \( x \),
optimizing towards \( x^* \) could reveal an image that the
neuron is looking for, a picture of the "concept" for \( h_{ij} \). When this 
optimization was done on a network trained to complete MNIST, they found that 
individual neurons at early layers (left) seemed to select for lines and curves in 
particular positions, whereas late-layer neurons were activated by entire digits. 


<img src="images/mnist_dbn_erhan.png" style="max-width:70%; width:500px;" class="mx-auto d-block">

<p>
Several modifications to the optimization algorithm were proposed by a series
of papers; an excellent feature visualization method that incorporates
many of the techniques was developed by <a href="https://distill.pub/2017/feature-visualization/"
>Chris Olah (2017)</a>, and was used to visualize all the units of several
networks at the <a href="https://microscope.openai.com/models">OpenAI Microscope</a>
website.

<h2>Network Dissection (Bau et al., 2020)</h2>

This paper from David's PhD work looks at neurons in two types of vision models: 
a CNN trained on scene classification and a GAN trained to generate scenes. 

<h3>Neurons in a CNN Scene Classifier</h3>

<h3>Neurons in a Generative Network</h3>



</main>
</div>
</div>
</body>
</html>

